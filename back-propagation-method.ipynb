{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a68c6b56",
   "metadata": {},
   "source": [
    "# 計算グラフ\n",
    "計算グラフとは、計算の過程をグラフによって表したもの。このグラフは複数のノードとそれをつなぐ直線（エッジ）で表される。ノードに演算内容を書き、計算結果が矢印の方向に流れていく。\n",
    "<br>\n",
    "<img src='https://camo.qiitausercontent.com/67dd08b85e6701a695d6e4a59ce5f7945e661161/68747470733a2f2f71696974612d696d6167652d73746f72652e73332e616d617a6f6e6177732e636f6d2f302f353631362f61393335366464632d626537392d346438322d356265642d3337396464393132666336632e706e67'>\n",
    "<br>\n",
    "計算グラフを用いて問題を解くには、\n",
    "1. 計算グラフの構築\n",
    "1. 計算グラフ上で左から右に計算を進める\n",
    "<br>\n",
    "\n",
    "この2つのステップが必要になる。この計算を左から右へ伝えるのを、順方向の伝播、略して**順伝播**と呼ぶ。また、右から左へ逆向きに伝播を行うことを**逆伝播**と言う。逆伝播が微分計算において重要な働きをする。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a081c945",
   "metadata": {},
   "source": [
    "## 計算グラフの利点\n",
    "計算グラフの利点は、「局所的な計算」を伝播することによって最終的な結果を得ることができることにある。どんな計算においても、各ノードでの「局所的な計算」を繰り返すことによって結果を得ることができる。これによって問題を単純化することができる。\n",
    "また、別の利点として、途中の計算結果を全て保持できることがあげられる。また、計算グラフの逆伝播によって、効率的に微分計算を行うことができるのがこの分野におけるこのグラフの最大の利点である。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0602e74",
   "metadata": {},
   "source": [
    "# 連鎖律\n",
    "逆伝播では、「局所的な微分」を、順方向とは逆向きに伝達していく。この「局所的な微分」を伝達する原理は、**連鎖律**(chain rule)によるものである。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fdeee76",
   "metadata": {},
   "source": [
    "## 計算グラフの逆伝播\n",
    "<img src='https://camo.qiitausercontent.com/90f1c5b0106a6aad600dee117755e85df25d5e9c/68747470733a2f2f71696974612d696d6167652d73746f72652e73332e616d617a6f6e6177732e636f6d2f302f353631362f32303033636231382d386239322d336135392d323165382d3463656330666639623736392e706e67'>\n",
    "<br>\n",
    "例えば、$y = f(x) = x^2$であるとき、逆伝播では入力$E$に$\\frac{\\partial y}{\\partial x}=2x$を乗算して前のノードに渡す。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0a27601",
   "metadata": {},
   "source": [
    "## 連鎖律とは\n",
    "$\n",
    "z = t^2\n",
    "\\\\\n",
    "t = x+y\n",
    "$\n",
    "といったように、複数の関数で構成される関数を合成関数という。（この場合は$z = (x+y)^2$を表す。）\n",
    "連鎖律は合成関数の微分についての性質であり、次のように定義される。\n",
    "\n",
    "> ある関数が合成関数で表される場合、その合成関数の微分は、合成関数を構成するそれぞれの関数の微分の積によって表すことができる。\n",
    "\n",
    "これを連鎖律の原理という。上記の例で数式に表すと以下のように表せる。\n",
    "\n",
    "$\\large \\frac{\\partial z}{\\partial x} = \\frac{\\partial z}{\\partial t} \\frac{\\partial t}{\\partial x}$\n",
    "<br>\n",
    "<br>\n",
    "これは、$\\partial t$が分子と分母で互いに打ち消し合うことから明らかである。上記の例では以下のように数式で表せる。\n",
    "<br>\n",
    "${\\large \\frac{\\partial z}{\\partial x} = \\frac{\\partial z}{\\partial t} \\frac{\\partial t}{\\partial x}}= 2t \\cdot 1 = 2(x+y)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c6992e1",
   "metadata": {},
   "source": [
    "## 連鎖律とグラフ\n",
    "上記の例の連鎖律の計算を、計算グラフで表してみる。2乗の演算を「\\*\\*2」というノードで表すと、以下のように書くことができる。\n",
    "<img src='https://cdn-ak.f.st-hatena.com/images/fotolife/l/losiz17/20180312/20180312192958.png'>\n",
    "このとき、1番最後の逆伝播の結果の部分では\n",
    "$\\large{\\frac{\\partial z}{\\partial z} \\frac{\\partial z}{\\partial t} \\frac{\\partial t}{\\partial x} = \\frac{\\partial z}{\\partial t} \\frac{\\partial t}{\\partial x} = \\frac{\\partial z}{\\partial x}}$が成り立ち、「$x$に関する$z$の微分」に対応している。このように、逆伝播の原理は連鎖律によって裏付けられている。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99e13c4e",
   "metadata": {},
   "source": [
    "# 逆伝播"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cd48a06",
   "metadata": {},
   "source": [
    "## 加算ノードの逆伝播\n",
    "$z = x+y$の微分について、以下のように解析的に計算することができる。\n",
    "<br>\n",
    "${\\large \\frac{\\partial z}{\\partial x}} = 1$\n",
    "<br>\n",
    "${\\large \\frac{\\partial z}{\\partial y}} = 1$\n",
    "<br>\n",
    "そのため、計算グラフ上の加算ノードでは単に上流から伝わった微分に1を乗算してそのまま下流のノードに流す。これを図で表すと次のようになる。\n",
    "<img src='https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.amazonaws.com%2F0%2F5616%2F93f921dd-17bb-5d7a-55a2-6173e1f62fe4.png?ixlib=rb-4.0.0&auto=format&gif-q=60&q=75&w=1400&fit=max&s=9257b2c9393d047d8c033c7c3f7c3c91'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "113da16d",
   "metadata": {},
   "source": [
    "## 乗算ノードの逆伝播\n",
    "$z=xy$という式について、この式の微分は次の式で表される。\n",
    "<br>\n",
    "${\\large \\frac{\\partial z}{\\partial x}} = y\n",
    "\\\\\n",
    "{\\large \\frac{\\partial z}{\\partial y}} = x$\n",
    "この式から、計算グラフは次のように書くことができる。\n",
    "<img src='https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.amazonaws.com%2F0%2F5616%2F32448b65-11cd-8935-849e-a21e23e3d4c4.png?ixlib=rb-4.0.0&auto=format&gif-q=60&q=75&w=1400&fit=max&s=42d05fd65c278d621e4391b99d883679'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dccc33da",
   "metadata": {},
   "source": [
    "これらのことから、最初に提示したリンゴの合計金額を表した計算グラフの逆伝播は、次のような図になる。\n",
    "<img src='graph.jpg'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16d1c242",
   "metadata": {},
   "source": [
    "# レイヤの実装\n",
    "実際に上記のリンゴの例をpythonで実装する。各レイヤ（ノード）は共通の```forword()```と```backword()```というメソッドを持つように実装する。（それぞれ順伝播と逆伝播に対応する。）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7471c85",
   "metadata": {},
   "source": [
    "## 乗算レイヤの実装\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "62af36c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MulLayer:\n",
    "    def __init__(self):\n",
    "        self.x = None\n",
    "        self.y = None\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y                \n",
    "        out = x * y\n",
    "\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dx = dout * self.y\n",
    "        dy = dout * self.x\n",
    "\n",
    "        return dx, dy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c09d866",
   "metadata": {},
   "source": [
    "コンストラクタで宣言しているx,yは順伝播時に入力値を保持しておき、逆伝播の演算時に使用するためのもの。\n",
    "<br>\n",
    "実際にこのレイヤを使用して、前項目のリンゴの代金の例を実装してみる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1daa1b6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "220\n"
     ]
    }
   ],
   "source": [
    "apple = 100\n",
    "apple_num = 2\n",
    "tax = 1.1\n",
    "\n",
    "#layer\n",
    "mul_apple_layer = MulLayer()\n",
    "mul_tax_layer = MulLayer()\n",
    "\n",
    "# forward\n",
    "apple_price = mul_apple_layer.forward(apple, apple_num)\n",
    "price = mul_tax_layer.forward(apple_price, tax)\n",
    "\n",
    "print(int(price))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0f20f5ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2 110 200\n"
     ]
    }
   ],
   "source": [
    "# backward\n",
    "dprice = 1\n",
    "dapple_price, dtax = mul_tax_layer.backward(dprice)\n",
    "dapple, dapple_num = mul_apple_layer.backward(dapple_price)\n",
    "\n",
    "print(dapple, int(dapple_num), dtax)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cb99aa8",
   "metadata": {},
   "source": [
    "## 加算レイヤの実装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "70a04b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AddLayer:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        out = x + y\n",
    "\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dx = dout * 1\n",
    "        dy = dout * 1\n",
    "\n",
    "        return dx, dy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcddafe2",
   "metadata": {},
   "source": [
    "今度はこのレイヤを使用して、以下のような図に表される計算グラフを実装してみる。\n",
    "<img src='https://camo.qiitausercontent.com/a360caa4521c95b32f192c8914c14005aec67a65/68747470733a2f2f71696974612d696d6167652d73746f72652e73332e616d617a6f6e6177732e636f6d2f302f353631362f64313633363031332d316339332d346364362d313961302d3933386230343763313335382e706e67'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "86d607a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "715\n",
      "110 2.2 3 165 650\n"
     ]
    }
   ],
   "source": [
    "apple = 100\n",
    "apple_num = 2\n",
    "orange = 150\n",
    "orange_num = 3\n",
    "tax = 1.1\n",
    "\n",
    "# layer\n",
    "mul_apple_layer = MulLayer()\n",
    "mul_orange_layer = MulLayer()\n",
    "add_apple_orange_layer = AddLayer()\n",
    "mul_tax_layer = MulLayer()\n",
    "\n",
    "# forward\n",
    "apple_price = mul_apple_layer.forward(apple, apple_num)  # (1)\n",
    "orange_price = mul_orange_layer.forward(orange, orange_num)  # (2)\n",
    "all_price = add_apple_orange_layer.forward(apple_price, orange_price)  # (3)\n",
    "price = mul_tax_layer.forward(all_price, tax)  # (4)\n",
    "\n",
    "# backward\n",
    "dprice = 1\n",
    "dall_price, dtax = mul_tax_layer.backward(dprice)  # (4)\n",
    "dapple_price, dorange_price = add_apple_orange_layer.backward(dall_price)  # (3)\n",
    "dorange, dorange_num = mul_orange_layer.backward(dorange_price)  # (2)\n",
    "dapple, dapple_num = mul_apple_layer.backward(dapple_price)  # (1)\n",
    "\n",
    "print(int(price))\n",
    "print(int(dapple_num), dapple, int(dorange), int(dorange_num), dtax)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bf0dca3",
   "metadata": {},
   "source": [
    "# 活性化関数レイヤの実装\n",
    "計算グラフの考え方をニューラルネットワークに応用していく。\n",
    "まずは、活性化関数であるReLUとSigmoidを実装する。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ea08b53",
   "metadata": {},
   "source": [
    "## ReLUレイヤ\n",
    "ReLU関数は次のような式で表される。\n",
    "<br>\n",
    "<br>\n",
    "$y = \n",
    "\\left \\{\n",
    "\\begin{array}{l}\n",
    "x　( x > 0) \\\\\n",
    "0　( x \\leqq 0)\n",
    "\\end{array}\n",
    "\\right.$\n",
    "<br>\n",
    "<br>\n",
    "よって、$x$に関する$y$の微分は以下のように求められる。\n",
    "<br>\n",
    "<br>\n",
    "${\\large \\frac{\\partial y}{\\partial x}} = \n",
    "\\left \\{\n",
    "\\begin{array}{l}\n",
    "1　( x > 0) \\\\\n",
    "0　( x \\leqq 0)\n",
    "\\end{array}\n",
    "\\right.$\n",
    "<br>\n",
    "<br>\n",
    "これを実装すると以下のようになる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c946e3f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Relu:\n",
    "    def __init__(self):\n",
    "        self.mask = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.mask = (x <= 0)\n",
    "        out = x.copy()\n",
    "        out[self.mask] = 0\n",
    "\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dout[self.mask] = 0\n",
    "        dx = dout\n",
    "\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e82dc728",
   "metadata": {},
   "source": [
    "ReLUレイヤはスイッチのような役割をしている。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76e229ec",
   "metadata": {},
   "source": [
    "## Sigmoidレイヤ\n",
    "シグモイド関数は以下の式で表される。\n",
    "<br>\n",
    "<br>\n",
    "$y = {\\large \\frac{1}{1 + \\exp(-x)}}$\n",
    "<br>\n",
    "<br>\n",
    "これを計算グラフで表すと次の図となる。\n",
    "<img src='https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.amazonaws.com%2F0%2F5616%2F0353ba71-c937-bdda-4c7b-ae21137eb1aa.png?ixlib=rb-4.0.0&auto=format&gif-q=60&q=75&s=defab7c336c8c44cab758dc5ddf32783'>\n",
    "<br>\n",
    "「exp」ノードでは$y = \\exp(x)$の計算を行い、「/」ノードでは$y = {\\large \\frac{1}{x}}$の計算を行う。\n",
    "<br>\n",
    "「exp」ノードに関して、微分は解析的に以下のように計算できる。\n",
    "<br>\n",
    "${\\large \\frac{\\partial y}{\\partial x}} = \\exp(x)$\n",
    "<br>\n",
    "<br>\n",
    "また、「/」ノードに関しても以下のように計算できる。\n",
    "<br>\n",
    "$\\begin{align}\\frac{\\partial y}{\\partial x} &= -\\frac{1}{x^2} \\\\ &= -y^2 \\end{align}$\n",
    "<br>\n",
    "よって、このグラフの逆伝播を計算グラフに表すと以下のようになる。\n",
    "<img src='https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.amazonaws.com%2F0%2F5616%2F31147beb-58cd-b452-bb76-fab8e4f792f6.png?ixlib=rb-4.0.0&auto=format&gif-q=60&q=75&s=e0901d96504e7ae0ccb0067cf3d57cdb'>\n",
    "これを簡略化して、このようになる。\n",
    "<img src='https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.amazonaws.com%2F0%2F5616%2F0c2cec9f-e5d9-54fb-ff93-23d896091d2c.png?ixlib=rb-4.0.0&auto=format&gif-q=60&q=75&w=1400&fit=max&s=0b2db0a76b2ccabc1ddfd0165b7d3c1e'>\n",
    "また、さらにここからこのように式を変形できる。\n",
    "<br>\n",
    "$\\begin{align}\n",
    "\\frac{\\partial L}{\\partial y}y^2\\exp(-x) &= \\frac{\\partial L}{\\partial y} \\frac{1}{(1+\\exp(-x))^2}\\exp(-x)\\\\\n",
    "&= \\frac{\\partial L}{\\partial y} \\frac{1}{1+\\exp(-x)} \\frac{\\exp(-x)}{1+\\exp(-x)}\\\\\n",
    "&= \\frac{\\partial L}{\\partial y} y(1-y)\n",
    "\\end{align}$\n",
    "<br>\n",
    "<br>\n",
    "これらを元に、Sigmoidレイヤをpythonで実装すると以下のようになる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "326ee8c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sigmoid:\n",
    "    def __init__(self):\n",
    "        self.out = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = sigmoid(x)\n",
    "        self.out = out\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dx = dout * (1.0 - self.out) * self.out\n",
    "\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acdaa20a",
   "metadata": {},
   "source": [
    "このコードでは、順伝播時に出力をインスタンス変数```out```に保存しておき、それを用いて逆伝播の計算を行う。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ae98bb4",
   "metadata": {},
   "source": [
    "# Affine/Softmaxレイヤの実装"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34ffd73d",
   "metadata": {},
   "source": [
    "## Affineレイヤ\n",
    "ニューラルネットワークの順伝播では、重み付き信号の総和の計算に行列の積計算を用いた。ニューラルネットワークの順伝播で行う行列の積計算を、幾何学の分野では「アフィン変換」と呼ぶ。そのため、ここではアフィン変換の処理を「Affineレイヤ」として実装する。\n",
    "\n",
    "$X \\cdot W = Y$\n",
    "\n",
    "<br>\n",
    "この式について、今までのスカラー値に対する計算と同じ手順で逆伝播について考えると、以下の式が得られる。\n",
    "\n",
    "$\n",
    "{\\large\\frac{\\partial L}{\\partial X}} = {\\large\\frac{\\partial L}{\\partial Y}} \\cdot W^T \\\\\n",
    "{\\large\\frac{\\partial L}{\\partial W}} = X^T \\cdot {\\large\\frac{\\partial L}{\\partial Y}}\n",
    "$\n",
    "\n",
    "<br>\n",
    "$W^T$のTは転置を表す。\n",
    "このとき特に、$X$と$\\frac{\\partial L}{\\partial X}$、$W$と$\\frac{\\partial L}{\\partial W}$はそれぞれ同じ形状である。\n",
    "<br>\n",
    "バッチ処理に対応する形に実装すると以下のようになる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ab7444ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "class Affine:\n",
    "    def __init__(self, W, b):\n",
    "        self.W =W\n",
    "        self.b = b\n",
    "        \n",
    "        self.x = None\n",
    "        self.dW = None\n",
    "        self.db = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.x = x\n",
    "        out = np.dot(self.x, self.W) + self.b\n",
    "\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dx = np.dot(dout, self.W.T)\n",
    "        self.dW = np.dot(self.x.T, dout)\n",
    "        self.db = np.sum(dout, axis=0)\n",
    "        \n",
    "        return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a700045",
   "metadata": {},
   "source": [
    "## Softmax-with-Lossレイヤ\n",
    "ここでは、損失関数である交差エントロピー誤差(cross entropy error)も含めて、「Softmax-with-Lossレイヤ」という名前で実装する。\n",
    "このとき、最終的な計算グラフは次のようになる。\n",
    "<img src='https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FrWMeM%2FbtqQptySbcy%2FOcmx41ncd8SD6e7nPhVAkK%2Fimg.png'>\n",
    "これを見ると分かるとおり、逆伝播における出力は現在のニューラルネットワークの出力と教師データの差となっている。これにより、ニューラルネットワークの出力の誤差が大きければ大きいほど、逆伝播では前のレイヤが学習する内容も大きくなる。\n",
    "<br>\n",
    "これを実装すると以下のようになる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fdbd229d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from common.functions import *\n",
    "\n",
    "class SoftmaxWithLoss:\n",
    "    def __init__(self):\n",
    "        self.loss = None\n",
    "        self.y = None # softmaxの出力\n",
    "        self.t = None # 教師データ\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        self.t = t\n",
    "        self.y = softmax(x)\n",
    "        self.loss = cross_entropy_error(self.y, self.t)\n",
    "        \n",
    "        return self.loss\n",
    "\n",
    "    def backward(self, dout=1):\n",
    "        batch_size = self.t.shape[0]\n",
    "        if self.t.size == self.y.size: # 教師データがone-hot-vectorの場合\n",
    "            dx = (self.y - self.t) / batch_size\n",
    "        else:\n",
    "            dx = self.y.copy()\n",
    "            dx[np.arange(batch_size), self.t] -= 1\n",
    "            dx = dx / batch_size\n",
    "        \n",
    "        return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e29eda38",
   "metadata": {},
   "source": [
    "# 誤差逆伝播法の実装"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a834940e",
   "metadata": {},
   "source": [
    "## 誤差逆伝播法に対応したニューラルネットワークの実装\n",
    "先週に学習した2層のニューラルネットワークを誤差逆伝播法に対応するよう改変する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cce7d331",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from common.layers import *\n",
    "from common.gradient import numerical_gradient\n",
    "from collections import OrderedDict\n",
    "\n",
    "class TwoLayerNet:\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, output_size, weight_init_std = 0.01):\n",
    "        # 重みの初期化\n",
    "        self.params = {}\n",
    "        self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_size)\n",
    "        self.params['b1'] = np.zeros(hidden_size)\n",
    "        self.params['W2'] = weight_init_std * np.random.randn(hidden_size, output_size) \n",
    "        self.params['b2'] = np.zeros(output_size)\n",
    "\n",
    "        # レイヤの生成\n",
    "        self.layers = OrderedDict()\n",
    "        self.layers['Affine1'] = Affine(self.params['W1'], self.params['b1'])\n",
    "        self.layers['Relu1'] = Relu()\n",
    "        self.layers['Affine2'] = Affine(self.params['W2'], self.params['b2'])\n",
    "\n",
    "        self.lastLayer = SoftmaxWithLoss()\n",
    "        \n",
    "    def predict(self, x):\n",
    "        for layer in self.layers.values():\n",
    "            x = layer.forward(x)\n",
    "        \n",
    "        return x\n",
    "        \n",
    "    # x:入力データ, t:教師データ\n",
    "    def loss(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        return self.lastLayer.forward(y, t)\n",
    "    \n",
    "    def accuracy(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        y = np.argmax(y, axis=1)\n",
    "        if t.ndim != 1 : t = np.argmax(t, axis=1)\n",
    "        \n",
    "        accuracy = np.sum(y == t) / float(x.shape[0])\n",
    "        return accuracy\n",
    "        \n",
    "    # x:入力データ, t:教師データ\n",
    "    def numerical_gradient(self, x, t):\n",
    "        loss_W = lambda W: self.loss(x, t)\n",
    "        \n",
    "        grads = {}\n",
    "        grads['W1'] = numerical_gradient(loss_W, self.params['W1'])\n",
    "        grads['b1'] = numerical_gradient(loss_W, self.params['b1'])\n",
    "        grads['W2'] = numerical_gradient(loss_W, self.params['W2'])\n",
    "        grads['b2'] = numerical_gradient(loss_W, self.params['b2'])\n",
    "        \n",
    "        return grads\n",
    "        \n",
    "    def gradient(self, x, t):\n",
    "        # forward\n",
    "        self.loss(x, t)\n",
    "\n",
    "        # backward\n",
    "        dout = 1\n",
    "        dout = self.lastLayer.backward(dout)\n",
    "        \n",
    "        layers = list(self.layers.values())\n",
    "        layers.reverse()\n",
    "        for layer in layers:\n",
    "            dout = layer.backward(dout)\n",
    "\n",
    "        # 設定\n",
    "        grads = {}\n",
    "        grads['W1'], grads['b1'] = self.layers['Affine1'].dW, self.layers['Affine1'].db\n",
    "        grads['W2'], grads['b2'] = self.layers['Affine2'].dW, self.layers['Affine2'].db\n",
    "\n",
    "        return grads"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9acc2743",
   "metadata": {},
   "source": [
    "```OrderedDict```には、順伝播で処理したレイヤの順番を記録し、逆伝播のときにその逆の順番をなぞっている。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b4dea18",
   "metadata": {},
   "source": [
    "## 誤差逆伝播法の勾配確認\n",
    "誤差逆伝播法は効率的に計算できる一方、実装が複雑でありミスが起きやすい。そのため、実装が簡単である数値微分の結果と比較して一致することを確認することがある。この作業を**勾配確認**と言う。実装は以下に示す。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7888b293",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1:4.4295685577171375e-10\n",
      "b1:2.3793310187857856e-09\n",
      "W2:5.558634955766584e-09\n",
      "b2:1.4058574143160918e-07\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from dataset.mnist import load_mnist\n",
    "\n",
    "# データの読み込み\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n",
    "\n",
    "network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n",
    "\n",
    "x_batch = x_train[:3]\n",
    "t_batch = t_train[:3]\n",
    "\n",
    "grad_numerical = network.numerical_gradient(x_batch, t_batch)\n",
    "grad_backprop = network.gradient(x_batch, t_batch)\n",
    "\n",
    "for key in grad_numerical.keys():\n",
    "    diff = np.average( np.abs(grad_backprop[key] - grad_numerical[key]) )\n",
    "    print(key + \":\" + str(diff))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72094136",
   "metadata": {},
   "source": [
    "それぞれとても小さい値になっていることから、十分な精度で実装されていることが分かる。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09b11e5b",
   "metadata": {},
   "source": [
    "## 誤差逆伝播法を使った学習\n",
    "誤差逆伝播法を用いて学習を行う実装をする。先週学んだコードの数値微分の部分を誤差逆伝播法に置き換えるだけである。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "344e898f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.11721666666666666 0.1174\n",
      "0.90255 0.9065\n",
      "0.9247666666666666 0.9266\n",
      "0.9381 0.9367\n",
      "0.9455333333333333 0.9442\n",
      "0.95275 0.9496\n",
      "0.9571833333333334 0.9534\n",
      "0.9611666666666666 0.9577\n",
      "0.96385 0.96\n",
      "0.9688 0.9624\n",
      "0.9692 0.9625\n",
      "0.9716833333333333 0.9638\n",
      "0.9742333333333333 0.9669\n",
      "0.9749166666666667 0.9665\n",
      "0.9745333333333334 0.9662\n",
      "0.9772666666666666 0.9685\n",
      "0.9779333333333333 0.9681\n"
     ]
    }
   ],
   "source": [
    "# データの読み込み\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n",
    "\n",
    "network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n",
    "\n",
    "iters_num = 10000\n",
    "train_size = x_train.shape[0]\n",
    "batch_size = 100\n",
    "learning_rate = 0.1\n",
    "\n",
    "train_loss_list = []\n",
    "train_acc_list = []\n",
    "test_acc_list = []\n",
    "\n",
    "iter_per_epoch = max(train_size / batch_size, 1)\n",
    "\n",
    "for i in range(iters_num):\n",
    "    batch_mask = np.random.choice(train_size, batch_size)\n",
    "    x_batch = x_train[batch_mask]\n",
    "    t_batch = t_train[batch_mask]\n",
    "    \n",
    "    # 勾配\n",
    "    #grad = network.numerical_gradient(x_batch, t_batch)\n",
    "    grad = network.gradient(x_batch, t_batch)\n",
    "    \n",
    "    # 更新\n",
    "    for key in ('W1', 'b1', 'W2', 'b2'):\n",
    "        network.params[key] -= learning_rate * grad[key]\n",
    "    \n",
    "    loss = network.loss(x_batch, t_batch)\n",
    "    train_loss_list.append(loss)\n",
    "    \n",
    "    if i % iter_per_epoch == 0:\n",
    "        train_acc = network.accuracy(x_train, t_train)\n",
    "        test_acc = network.accuracy(x_test, t_test)\n",
    "        train_acc_list.append(train_acc)\n",
    "        test_acc_list.append(test_acc)\n",
    "        print(train_acc, test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dadf643",
   "metadata": {},
   "source": [
    "このように、数値微分では処理が非効率的で時間がかかった処理も、誤差逆伝播法を用いることで効率的に処理できた。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67aefda5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
